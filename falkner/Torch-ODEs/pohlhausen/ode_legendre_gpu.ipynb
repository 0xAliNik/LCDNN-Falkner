{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUikrwurWg6O",
        "outputId": "5bccc6d8-9466-4341-c30e-091979442876"
      },
      "outputs": [],
      "source": [
        "!apt install swig\n",
        "!pip install orthnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uGSiDN4bVyDe"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch import randn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch import nn\n",
        "\n",
        "import random, os\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "from orthnet import Legendre, Chebyshev\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "GPU_MODE = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh9WdW5ofFuc",
        "outputId": "91e323fa-9b3a-4243-d4c3-e4a4b17482e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 NVIDIA GeForce GTX 1070\n"
          ]
        }
      ],
      "source": [
        "if GPU_MODE:\n",
        "  cuda0 = torch.device('cuda:0')\n",
        "  print(torch.cuda.device_count(), torch.cuda.get_device_name(0))\n",
        "  torch.cuda.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pFyCR5IXVyDq"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w8taawEZVyDs"
      },
      "outputs": [],
      "source": [
        "class LegendreActivation(nn.Module):\n",
        "    def __init__(self,degree):\n",
        "        super().__init__()\n",
        "        self.degree = degree\n",
        "        self.data = None\n",
        "        \n",
        "        self.D = torch.zeros((degree , degree )) \n",
        "        for i in range(degree):\n",
        "          for j in range(0 , i):\n",
        "            if (i + j) % 2 == 1:\n",
        "              self.D[i, j] = 2 * (j + 1) - 1\n",
        "    \n",
        "    def forward(self, X):              \n",
        "      data = Legendre(X, self.degree).tensor\n",
        "      self.data = data\n",
        "      return data\n",
        "\n",
        "    def backward(self,):\n",
        "      return (self.D @ (self.data).T).T\n",
        "\n",
        "\n",
        "class LegendreBlock(nn.Module):\n",
        "    def __init__(self, n_input, degree):        \n",
        "        super().__init__()\n",
        "        self.degree = degree - 1\n",
        "        self.n_input = n_input\n",
        "        self.linear = nn.Linear(self.n_input, 1).double()\n",
        "        self.tanh = nn.Tanh().double()\n",
        "        self.Legendre = LegendreActivation(self.degree)\n",
        "\n",
        "    def forward(self, X):      \n",
        "      X = self.tanh(self.linear(X))      \n",
        "      data = self.Legendre(X)\n",
        "      return data\n",
        "\n",
        "class ChebyshevActivation(nn.Module):\n",
        "    def __init__(self,degree):\n",
        "        super().__init__()\n",
        "        self.degree = degree\n",
        "        self.data = None\n",
        "        \n",
        "        self.D = torch.zeros((degree , degree )) \n",
        "        for i in range(degree):\n",
        "          for j in range(0 ,i):\n",
        "            if (i+j) % 2 == 1:\n",
        "              self.D[i, j] = 2 * i\n",
        "              if j == 0:\n",
        "                self.D[i, j] = self.D[i, j]/2.0\n",
        "    def forward(self, X):              \n",
        "      data = Chebyshev(X, self.degree).tensor\n",
        "      self.data = data\n",
        "      return data\n",
        "\n",
        "    def backward(self,):\n",
        "      return (self.D @ (self.data).T).T\n",
        "\n",
        "class ChebyshevBlock(nn.Module):\n",
        "    def __init__(self, n_input, degree):        \n",
        "        super().__init__()\n",
        "        self.degree = degree - 1\n",
        "        \n",
        "        self.n_input = n_input\n",
        "        self.linear = nn.Linear(self.n_input, 1).double()\n",
        "        self.tanh = nn.Tanh().double()\n",
        "        self.Chebyshev = ChebyshevActivation(self.degree)\n",
        "\n",
        "\n",
        "    def forward(self, X):      \n",
        "      X = self.tanh(self.linear(X))      \n",
        "      data = self.Chebyshev(X)\n",
        "      \n",
        "      return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HSCVIWphVyDw"
      },
      "outputs": [],
      "source": [
        "def dy_dx(y, x):\n",
        "  return torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
        "\n",
        "def d2y_dx2(y, x):\n",
        "  return dy_dx(dy_dx(y,x), x)\n",
        "\n",
        "def d3y_dx3(y, x):\n",
        "  return dy_dx(d2y_dx2(y,x), x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TfdH0fFtVyDy"
      },
      "outputs": [],
      "source": [
        "domain = [0, 6]\n",
        "n_discretization = 3000 * domain[1] - domain[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uItLWMf5VyDz"
      },
      "outputs": [],
      "source": [
        "n_input = 1\n",
        "n_output = 1\n",
        "eps = 1e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7RRGAEaeVyD0"
      },
      "outputs": [],
      "source": [
        "if GPU_MODE:\n",
        "  x = torch.linspace(domain[0] + eps, domain[1] - eps, n_discretization, dtype=torch.double, device=cuda0).reshape(-1,1)\n",
        "  x = Variable(x, requires_grad=True).double()\n",
        "else:\n",
        "  x = torch.linspace(domain[0] + eps, domain[1] - eps, n_discretization, dtype=torch.double).reshape(-1,1)\n",
        "  x = Variable(x, requires_grad=True).double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0jsyeH_VyD1",
        "outputId": "aed38bab-36f3-4948-8b0d-cdcf9c6d0e0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp = nn.Sequential(  \n",
        "  LegendreBlock(n_input, 16),\n",
        "  nn.Linear(16, n_output)\n",
        ").double()\n",
        "\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in mlp.parameters() if p.requires_grad)\n",
        "pytorch_total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5MjbANFiiS_h"
      },
      "outputs": [],
      "source": [
        "if GPU_MODE:\n",
        "  mlp.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O7fJt8lWVyD3"
      },
      "outputs": [],
      "source": [
        "def get_loss(x, ret_res=False):\n",
        "  y = mlp(x)\n",
        "  y_x = dy_dx(y, x)\n",
        "  y_xx = dy_dx(y_x, x)\n",
        "  y_xxx = dy_dx(y_xx, x)\n",
        "\n",
        "\n",
        "  \"\"\"    \n",
        "  Generall Falkner-Skan Eq.: \n",
        "    f''' + α ff'' + β(1 - (f')^2) = 0 ,\n",
        "    f(0) = f'(0) = 0, f'(∞) = 1\n",
        "  \n",
        "  Pohlhausen Flow = (α, β) = (0, 1)  \n",
        "  \"\"\"\n",
        "  alpha = 0\n",
        "  beta = 1\n",
        "\n",
        "  residual = (y_xxx) + (alpha * y_xx * y) + (beta * (1 - y_x**2))\n",
        "\n",
        "\n",
        "  # boundaries same for all equations\n",
        "  boundary1 = y[0]\n",
        "  boundary2 = y_x[0]\n",
        "  boundary3 = y_x[-1] - 1\n",
        "\n",
        "  loss = (residual**2).mean() + boundary1**2 + boundary2**2 + boundary3**2\n",
        "  return (loss, residual) if ret_res else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2H9IZlCQVyD4"
      },
      "outputs": [],
      "source": [
        "def closure():\n",
        "  loss = get_loss(x)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bOqRqaiVyD5",
        "outputId": "5d6230bc-b68d-4a15-8037-b7502aa9aabb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 000 loss = 00000.0000201491\n",
            "Step: 002 loss = 00000.0975390748\n",
            "Step: 004 loss = 00000.5284024672\n",
            "Step: 006 loss = 00000.8275130308\n",
            "Step: 008 loss = 00000.7128033487\n",
            "Step: 010 loss = 00000.5043703211\n",
            "Step: 012 loss = 00000.4433624040\n",
            "Step: 014 loss = 00000.4133277524\n",
            "Step: 016 loss = 00000.2797852705\n",
            "Step: 018 loss = 00000.2390201877\n",
            "Step: 020 loss = 00000.2838363563\n",
            "Step: 022 loss = 00000.2789480085\n",
            "Step: 024 loss = 00000.2278492039\n",
            "Step: 026 loss = 00000.1942393628\n",
            "Step: 028 loss = 00000.1513490099\n",
            "Step: 030 loss = 00000.1032854993\n",
            "Step: 032 loss = 00000.0780683005\n",
            "Step: 034 loss = 00000.0485425857\n",
            "Step: 036 loss = 00000.0108489768\n",
            "Step: 038 loss = 00000.0259125854\n",
            "Step: 040 loss = 00000.0441765166\n",
            "Step: 042 loss = 00000.0193286189\n",
            "Step: 044 loss = 00000.0251525842\n",
            "Step: 046 loss = 00000.0269919095\n",
            "Step: 048 loss = 00000.0119713746\n",
            "Step: 050 loss = 00000.0090503400\n",
            "Step: 052 loss = 00000.0124085630\n",
            "Step: 054 loss = 00000.0063530951\n",
            "Step: 056 loss = 00000.0048484846\n",
            "Step: 058 loss = 00000.0081278258\n",
            "Step: 060 loss = 00000.0074811043\n",
            "Step: 062 loss = 00000.0048551747\n",
            "Step: 064 loss = 00000.0048478575\n",
            "Step: 066 loss = 00000.0036391461\n",
            "Step: 068 loss = 00000.0017271817\n",
            "Step: 070 loss = 00000.0023642568\n",
            "Step: 072 loss = 00000.0029300684\n",
            "Step: 074 loss = 00000.0022006496\n",
            "Step: 076 loss = 00000.0022902514\n",
            "Step: 078 loss = 00000.0019084455\n",
            "Step: 080 loss = 00000.0011004218\n",
            "Step: 082 loss = 00000.0013947383\n",
            "Step: 084 loss = 00000.0012605611\n",
            "Step: 086 loss = 00000.0011044936\n",
            "Step: 088 loss = 00000.0012145612\n",
            "Step: 090 loss = 00000.0009493671\n",
            "Step: 092 loss = 00000.0008483686\n",
            "Step: 094 loss = 00000.0008123492\n",
            "Step: 096 loss = 00000.0007026449\n",
            "Step: 098 loss = 00000.0006631726\n",
            "Step: 100 loss = 00000.0006618321\n",
            "Step: 102 loss = 00000.0006312399\n",
            "Step: 104 loss = 00000.0005999391\n",
            "Step: 106 loss = 00000.0005785282\n",
            "Step: 108 loss = 00000.0005570116\n",
            "Step: 110 loss = 00000.0005068891\n",
            "Step: 112 loss = 00000.0003493455\n",
            "Step: 114 loss = 00000.0001694091\n",
            "Step: 116 loss = 00000.0000832682\n",
            "Step: 118 loss = 00000.0000476978\n",
            "Step: 120 loss = 00000.0000329539\n",
            "Step: 122 loss = 00000.0000266479\n",
            "Step: 124 loss = 00000.0000238560\n",
            "Step: 126 loss = 00000.0000225859\n",
            "Step: 128 loss = 00000.0000219299\n",
            "Step: 130 loss = 00000.0000216068\n",
            "Step: 132 loss = 00000.0000214630\n",
            "Step: 134 loss = 00000.0000213995\n",
            "Step: 136 loss = 00000.0000213965\n",
            "Step: 138 loss = 00000.0000213946\n",
            "Step: 140 loss = 00000.0000213928\n",
            "Step: 142 loss = 00000.0000213910\n",
            "Step: 144 loss = 00000.0000213893\n",
            "Step: 146 loss = 00000.0000213877\n",
            "Step: 148 loss = 00000.0000213862\n",
            "Step: 150 loss = 00000.0000213847\n",
            "Step: 152 loss = 00000.0000213833\n",
            "Step: 154 loss = 00000.0000213819\n",
            "Step: 156 loss = 00000.0000213806\n",
            "Step: 158 loss = 00000.0000213793\n",
            "Step: 160 loss = 00000.0000213781\n",
            "Step: 162 loss = 00000.0000213769\n",
            "Step: 164 loss = 00000.0000213758\n",
            "Step: 166 loss = 00000.0000213747\n",
            "Step: 168 loss = 00000.0000213737\n",
            "Step: 170 loss = 00000.0000213727\n",
            "Step: 172 loss = 00000.0000213718\n",
            "Step: 174 loss = 00000.0000213708\n",
            "Step: 176 loss = 00000.0000213700\n",
            "Step: 178 loss = 00000.0000213691\n",
            "Step: 180 loss = 00000.0000213683\n",
            "Step: 182 loss = 00000.0000213675\n",
            "Step: 184 loss = 00000.0000213668\n",
            "Step: 186 loss = 00000.0000213661\n",
            "Step: 188 loss = 00000.0000213654\n",
            "Step: 190 loss = 00000.0000213647\n",
            "Step: 192 loss = 00000.0000213641\n",
            "Step: 194 loss = 00000.0000213634\n",
            "Step: 196 loss = 00000.0000213629\n",
            "Step: 198 loss = 00000.0000213623\n",
            "Step: 200 loss = 00000.0000213617\n",
            "Step: 202 loss = 00000.0000213612\n",
            "Step: 204 loss = 00000.0000213607\n",
            "Step: 206 loss = 00000.0000213602\n",
            "Step: 208 loss = 00000.0000213598\n",
            "Step: 210 loss = 00000.0000213593\n",
            "Step: 212 loss = 00000.0000213589\n",
            "Step: 214 loss = 00000.0000213585\n",
            "Step: 216 loss = 00000.0000213581\n",
            "Step: 218 loss = 00000.0000213577\n",
            "Step: 220 loss = 00000.0000213573\n",
            "Step: 222 loss = 00000.0000213570\n",
            "Step: 224 loss = 00000.0000213566\n",
            "Step: 226 loss = 00000.0000213563\n",
            "Step: 228 loss = 00000.0000213560\n",
            "Step: 230 loss = 00000.0000213557\n",
            "Step: 232 loss = 00000.0000213554\n",
            "Step: 234 loss = 00000.0000213551\n",
            "Step: 236 loss = 00000.0000213548\n",
            "Step: 238 loss = 00000.0000213545\n",
            "Step: 240 loss = 00000.0000213543\n",
            "Step: 242 loss = 00000.0000213541\n",
            "Step: 244 loss = 00000.0000213538\n",
            "Step: 246 loss = 00000.0000213536\n",
            "Step: 248 loss = 00000.0000213534\n",
            "Step: 250 loss = 00000.0000213532\n",
            "Step: 252 loss = 00000.0000213530\n",
            "Step: 254 loss = 00000.0000213528\n",
            "Step: 256 loss = 00000.0000213526\n",
            "Step: 258 loss = 00000.0000213524\n",
            "Step: 260 loss = 00000.0000213522\n",
            "Step: 262 loss = 00000.0000213521\n",
            "Step: 264 loss = 00000.0000213519\n",
            "Step: 266 loss = 00000.0000213517\n",
            "Step: 268 loss = 00000.0000213516\n",
            "Step: 270 loss = 00000.0000213514\n",
            "Step: 272 loss = 00000.0000213513\n",
            "Step: 274 loss = 00000.0000213512\n",
            "Step: 276 loss = 00000.0000213510\n",
            "Step: 278 loss = 00000.0000213509\n",
            "Step: 280 loss = 00000.0000213508\n",
            "Step: 282 loss = 00000.0000213507\n",
            "Step: 284 loss = 00000.0000213505\n",
            "Step: 286 loss = 00000.0000213504\n",
            "Step: 288 loss = 00000.0000213503\n",
            "Step: 290 loss = 00000.0000213502\n",
            "Step: 292 loss = 00000.0000213501\n",
            "Step: 294 loss = 00000.0000213500\n",
            "converged\n",
            "Final loss = 2.14e-05\n"
          ]
        }
      ],
      "source": [
        "#TODO\n",
        "optimizer = optim.Adam(list(mlp.parameters()), lr=0.05, betas=(0.9, 0.999), eps=1e-32)\n",
        "previous = 0\n",
        "losses = []\n",
        "epoch_Adam = 100\n",
        "epoch_LBFGS = 10000\n",
        "for i in range(epoch_Adam):\n",
        "  loss = get_loss(x)\n",
        "  \n",
        "  if i % 2 == 0:        \n",
        "    print('Step: %03d loss = %016.10f' % (i, loss))        \n",
        "  \n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "optimizer = optim.LBFGS(list(mlp.parameters()), lr = 0.01)\n",
        "previous = 0\n",
        "for i in range(epoch_LBFGS):\n",
        "  loss = get_loss(x)\n",
        "  if i % 2 == 0:        \n",
        "    print('Step: %03d loss = %016.10f' % (i+epoch_Adam, loss))\n",
        "    if abs(previous - loss) < 1e-10:\n",
        "        print('converged')\n",
        "        break\n",
        "    \n",
        "    previous = loss\n",
        "  \n",
        "  losses.append(loss.detach().cpu().numpy())\n",
        "  optimizer.step(closure)\n",
        "\n",
        "print(\"Final loss = %.2e\" % get_loss(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdmgRC0AVyD6",
        "outputId": "e1b4e2cb-63ae-48cc-b2b9-2dcf2d340a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f''(0) = 1.154779504589012\n"
          ]
        }
      ],
      "source": [
        "# calculate f''(0)\n",
        "# # make zero Tensor from x Tensor\n",
        "zerox = x.clone()\n",
        "zerox[0][0] = 0\n",
        "f_xx_0 = d2y_dx2(mlp(zerox), zerox)[0]\n",
        "print(\"f''(0) = {}\".format(f_xx_0[0]))\n",
        "\n",
        "\n",
        "#  Exact = 1.154701"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "1i9w6wQuVyD8",
        "outputId": "69fcb7b6-e87a-4db8-e672-ddbcdf14ca27"
      },
      "outputs": [],
      "source": [
        "mlp.to('cpu')\n",
        "plt.figure(figsize=(10, 5))\n",
        "domain = x.cpu().detach().numpy().flatten()\n",
        "\n",
        "res= get_loss(x.cpu(), ret_res=True)[1].detach().numpy()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Residual')\n",
        "plt.plot(domain, res)\n",
        "plt.savefig('residual-loss.eps', bbox_inches='tight', format='eps')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "A4TEZCiWVyD9",
        "outputId": "ab8f7c75-2e1e-4911-c605-55e52653de88"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.log(losses))\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('log(loss)')\n",
        "\n",
        "plt.savefig('loss.eps', bbox_inches='tight', format='eps')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ode_legendre_chebyshev (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8c6a7a98d988520ca49873c8c8a84fc603de93e0e51e3f6cf3504553200cd803"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
